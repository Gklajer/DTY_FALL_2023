{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing dogs and cats\n",
    "\n",
    "The purpose of this laboratory is to build a first end to end reflex-based AI model to teach computers to [**understand images**](https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures).\n",
    "\n",
    "In particular, the objective of this lab is to write an AI application able to recognize cats and dogs on images. Your application will take an image as input and will be able to say wheter the image contains a dog or a cat. You will work with the data of the [**Dogs vs Cats**](https://www.kaggle.com/c/dogs-vs-cats) competition from Kaggle. This competition was launched in 2013 and the first place was obtained by [Pierre Sermanet](https://research.google.com/pubs/PierreSermanet.html), actually Research Scientist at Google Brain, by using the [Overfeat](http://cilvr.nyu.edu/doku.php?id=software:overfeat:start#overfeatobject_recognizer_feature_extractor) deep learning library he wrote during his PhD at New York University under the supervision of [Yann Le Cun](http://yann.lecun.com/), Director of AI Research at Facebook. He obtained $1.09%$ of classification errors. Try to do your best to approach this score!!!\n",
    "\n",
    "Two approaches will be used to adress this problem :\n",
    "1. A traditional pattern recognition model in which hand-crafted features are extracted from images and used to represent them and to train classifiers.\n",
    "2. A modern representation learning approach in which deep convolutional neural networks (CNN) are used to learn the image representations.\n",
    "\n",
    " \n",
    "\n",
    "##  Learning outcomes\n",
    "+ Building an end to end supervised machine learning pipeline : input data (training set) preparation, training (model learning), validation sets, cross-validation, hyper-parameter tuning, evaluation on the testing dataset.\n",
    "+ Getting familiar with deep learning for image classification : model building and training, transfer learning, fine-tuning.\n",
    "+ Getting familiar with some well-known librairies:\n",
    "    + Machine learning : Scikit-learn ([http://scikit-learn.org/stable/](http://scikit-learn.org/stable/))\n",
    "    + Deep learning: Keras ([https://keras.io/](https://keras.io/))\n",
    "    + Computer vision : Scikit-image ([http://scikit-image.org/](http://scikit-image.org/) or OpenCV ([http://opencv.org/](http://opencv.org/))\n",
    "    \n",
    "**The final objective of this laboratory is to be aware to the potential but also to the limitations of reflex-based AI approaches towards visual recognition tasks.**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 0 : Requirements\n",
    "A set of packages will be useful to handle the first part of this study case.\n",
    "\n",
    "    pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : A small tutorial on image classification\n",
    "\n",
    "In this section, we will briefly introduce the image classification problem which consists in assigning to an input image one label from a fixed set of labels and which is one of the big challenge of computer vision and artifial intelligence. In our case, we will only consider two labels $\\{dog, cat\\}$. This small tutorial also aims at familiarizing you with machine learning and computer vision librairies that we will used in this course :\n",
    "+ Scikit-Learn : [http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)\n",
    "+ OpenCV : [http://opencv.org/](http://opencv.org/) or Scikit-image ([http://scikit-image.org/](http://scikit-image.org/)).\n",
    "\n",
    "![ImageCat](images/Diapositive1.jpg)\n",
    "\n",
    "While the task of image classification is very easy for a human, we have to face with several challenge to build our automatic recognition algorithm among whom:\n",
    "\n",
    "+ Viewpoint variation.\n",
    "+ Scale variation.\n",
    "+ Illumination conditions variation.\n",
    "+ Deformation.\n",
    "+ Occlusion.\n",
    "+ Backgroud clutter.\n",
    "+ Intraclass variation.\n",
    "\n",
    "![ImageCatwithvariations](./images/Diapositive2.jpg)\n",
    "\n",
    "![ImageCatwithocclusion](./images/Diapositive3.jpg)\n",
    "\n",
    "Source : Images from the CS231n course of Stanford (Convolutional Neural Networks for Visual Recognition)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple image classification pipeline\n",
    "\n",
    "To built our image classification algorithm, we will follow the principle of a machine learning approach for image classification which consists in :\n",
    "1. Collecting and preparing a dataset of images and their corresponding labels.\n",
    "2. Using a machine learning algorithm to train a classifier.\n",
    "3. Evaluate the classifier on new images.\n",
    "\n",
    "\n",
    "![ImageClassificationpipeline](images/testphase.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having a look on the available data\n",
    "\n",
    "First, you have to  download the dataset that will be used to train and test our model. Before downloading the data, create a subdirectory in your working folder called data. Then download [DataDogsCatsChallenge.zip](https://filesender.renater.fr/?s=download&token=cd3d55af-2563-47c4-af3c-28a4eba661e7) into that directory and unzip it. This dataset contains 25,000 labelled dog and cat photos available for training, and 12,500 in the test set that we have to try to label for the Kaggle competition.\n",
    "\n",
    "As we have seen in the lecture note, the standard practice in machine learning is to split the available data into at least two different subsets :\n",
    "+ The **training set** : to learn the model.\n",
    "+ The **testing set** : to evaluate the learned model.\n",
    "\n",
    "You have also seen that is the also standard to add a third set to the split to build a **validation set** that will be used to fine-tune the parameters of the model.\n",
    "\n",
    "I you have a look on the DogsCatsChallenge directory, you will see that the preparation of the data have been done and that the test and train sets are in separate subdirectories in which data for each category (cats and dogs) is also into subdirectories. Nevertheless, there is no validation set and one of your first task will be to build it.\n",
    "\n",
    "The archive also contains a directory named **sample**. Training and validating the entire dataset can take some time. Therefore, it is a good practice to run first your algorithm on a small sample of your training and validation data before to run it on the entire set of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image representation\n",
    "\n",
    "Your first task will be to built a representation of the data, i.e. a feature vector which values quantify the contents of the image. We will see latter that, using Deep Convolutional Neural Networks, we can learn an efficient representation directly using raw pixel intensities as inputs. Here, we will just represent the images by two alternative representations.\n",
    "+ A first representation is built using the raw data by simply resizing an input image to a fixed size (here $32 \\times 32$ pixels) and then by flattening the RBG pixel intensities into a single vectors of numbers.\n",
    "+ A second representation is built from the color histogram that characterizes the color distribution of the image. For this representation a color conversion into the HSV color space could be useful.\n",
    "\n",
    "**Complete the functions below to build such representations**\n",
    "\n",
    "Some helping functions :\n",
    "+ With Numpy:\n",
    "    + [Array flattening](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html)\n",
    "    \n",
    "+ With OpenCV:\n",
    "\n",
    "     + [Geometric transformations on images](http://docs.opencv.org/trunk/da/d6e/tutorial_py_geometric_transformations.html) \n",
    "     + [Colorspace conversion](http://docs.opencv.org/trunk/df/d9d/tutorial_py_colorspaces.html) \n",
    "     + [Histogram in OpenCV](http://docs.opencv.org/trunk/de/db2/tutorial_py_table_of_contents_histograms.html)\n",
    "    \n",
    "+ With Scikit-image :\n",
    "    + [Loading an image from a file](https://scikit-image.org/docs/stable/user_guide/data_types.html)\n",
    "    + [Image transformations](https://scikit-image.org/docs/dev/user_guide/transforming_image_data.html)\n",
    "    + [Colorspace conversion](https://scikit-image.org/docs/dev/user_guide/transforming_image_data.html#color-manipulation)\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous charge les d√©pendances utiles pour la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Using Convolutional Neural Networks\n",
    "\n",
    "In this part, you are going to use convolutional neural networks (CNNs) and deep learning in order to build your image classifier. You will use the [Keras framework](https://keras.io/) which is a high-level neural networks API, written in Python and capable of running on top of [TensorFlow](https://www.tensorflow.org/), [CNTK](https://github.com/Microsoft/CNTK), or [Theano](http://www.deeplearning.net/software/theano/). It was developed with a focus on enabling fast experimentation and as a consequence it is a good choice for this course. Various other frameworks are available and can also be used such as [Caffe framework](http://caffe.berkeleyvision.org/), [Torch](http://torch.ch/) or [DeepLearning4j](https://deeplearning4j.org/) among others. Another important deep learning framework is [pytorch](https://pytorch.org/).\n",
    "\n",
    "\n",
    "### Keras with sample data from the Dogs and cats recognition challenge\n",
    "\n",
    "In this part, you will have to use Keras in order to\n",
    " + Build and train a small network from scratch\n",
    " + Use the bottleneck features of a pre-trained network\n",
    " + Fine-tune the top layers of a pre-trained network\n",
    "\n",
    "The work will be done on a **sample dataset** (sampleDeep) of the initial Kaggle challenge that contains a training set composed of 1000 images of cats and 1000 images of dogs and a validation set, used to evaluate our models, that contains 400 additional samples from each class.\n",
    "\n",
    "At the end, you could apply this approach on the whole dataset but it will imply to have other computing ressources than just your own computer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation and loading\n",
    "\n",
    "As for the previous classifiers, data preparation is also required when working with convolutional neural networks and deep learning models. You will use the [*ImageDataGenerator class*](https://keras.io/preprocessing/image/) that defines the configuration for image data preparation but also for data augmentation, a step often necessary for deep learning. In the code below, you will have to create and configure an `ImageDataGenerator` and to fit it on your data. In this example, we will use the sample dataset of the Dogs and Cats challenge. We consider that you have a training directory and a validation directory setup in this manner :\n",
    "\n",
    "    train_dir/\n",
    "        dog/\n",
    "        cat/\n",
    "    val_dir/\n",
    "        dog/\n",
    "        cat\n",
    " This is the case of the `sampleDeep` dataset.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without augmentation, only rescaling\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# definition of the number of samples propagated through the network at each step\n",
    "batch_size = 16\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/DogCatChallenge/sampleDeep/train'\n",
    "validation_data_dir = 'data/DogCatChallenge/sampleDeep/valid'\n",
    "\n",
    "# create and configure an ImageDataGenerator for the training data with only rescaling to 0..1\n",
    "train_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,  # the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size) \n",
    "\n",
    "# create and configure an ImageDataGenerator for the validation data with only rescaling to 0..1\n",
    "test_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "valid_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,  # the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only few training examples are available in the `sampleDeep` dataset. In order to make the most of these training examples, a current approach is to **augment** them via a number of random transformations, so that our model would never see twice the exact same picture. This augmentation step helps prevent overfitting and helps the model generalize better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with augmentation \n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# definition of the number of samples propagated through the network at each step\n",
    "batch_size = 16\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/DogCatChallenge/sampleDeep/train'\n",
    "validation_data_dir = 'data/DogCatChallenge/sampleDeep/valid'\n",
    "\n",
    "# Create and configure an ImageDataGenerator for the training data\n",
    "# TO DO :  augmentation of the training data using rotation, horizontal and vertical shift, shearing tranformation, zooming \n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=120,\n",
    "    width_shift_range=0.6,\n",
    "    height_shift_range=0.5,\n",
    "    brightness_range=[0.2,1.0],\n",
    "    shear_range=50,\n",
    "    zoom_range=[0.5,1.0],\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# TO DO : Create and configure an ImageDataGenerator for the validation data\n",
    "# Only rescaling for validation data\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# TO DO : generator that will read pictures found in the train dataset directory and that will indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,  # the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size\n",
    ")\n",
    "\n",
    "# TO DO : Similar generator for validation data\n",
    "valid_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,  # the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model from scratch\n",
    "\n",
    "Models can be build easily with the Keras API. Here we will use the Sequential model API :\n",
    "+ [https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/)\n",
    "\n",
    "\n",
    "Here, you will build a convolutional neural network which is ,by design, one of the best models available for most \"perceptual\" problems (such as image classification), even with very little data to learn from.\n",
    "\n",
    "In the code below, you have to build a model composed of 3 convolution layers with a ReLU activation and followed by max-pooling layers.\n",
    "In order to write the code, have a look on the [documentation on the different kinds of layers available in Keras](https://keras.io/layers/about-keras-layers/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "\n",
    "# Second  convolutional layer\n",
    "\n",
    "model.add(Conv2D(64, (5,5), activation='relu'))\n",
    "# on tente sans pooling pour ne pas trop r√©duire la taille de l'image\n",
    " \n",
    "# Third convolutional layer    \n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))   # pour √©viter l'overfitting\n",
    "\n",
    "# Adding of two fully-connected layers \n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))  # on aurait pu monter encore en termes de units\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "\n",
    "# single unit and sigmoid activation, which is perfect for a binary classification. \n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Use of the binary_crossentropy loss to train our model, of the rmsprop optimizer and the accuracy metrics\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a model\n",
    "\n",
    "It may be sometimes useful to visualize in a schematic way a model architecture. You can do it with different approaches on Keras :\n",
    "\n",
    "+ using the `plot_model` built-in function : see [this small tutorial](https://www.machinecurve.com/index.php/2019/10/07/how-to-visualize-a-model-with-keras/) or the [official documentation](https://keras.io/api/utils/model_plotting_utils/).\n",
    "+ using [TensorBoard](https://www.tensorflow.org/tensorboard) if you are using the tensorflow backend on Keras. You can also find some documentations on this small [tutorial](https://www.machinecurve.com/index.php/2019/12/03/visualize-keras-models-overview-of-visualization-methods-tools/#visualizing-model-architecture-tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "# TO DO\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='mymodel.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "\n",
    "We can now use some defined generators to train your build model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# augmentation configuration use for training:\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# augmentation configuration use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# fit the generator to your data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "# model training\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = 20,\n",
    "    epochs = 50,   # √©tant donn√© qu'on a peu de donn√©es...\n",
    "    verbose = 1,\n",
    "    validation_data = validation_generator,callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# saving the learned model\n",
    "model.save_weights('first_try.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carefully have a look on the results and on the diffferent metrics and their obtained values. What is your interpretation of the results ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now apply this model to any new image. For instance, in the code below, you have to apply the model on different images of the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from skimage import data, io\n",
    "from matplotlib.pyplot import imshow\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img_path = 'data/DogCatChallenge/test1/10001.jpg'\n",
    "\n",
    "ima=io.imread(img_path)\n",
    "imshow(ima)\n",
    "\n",
    "# image loading and transformation to keras\n",
    "image = image.img_to_array(image.load_img(img_path, target_size=(img_width, img_height))) / 255\n",
    "single_im = np.expand_dims(image, axis=0)\n",
    "\n",
    "# prediction on the image\n",
    "print(\"Prediction\")\n",
    "\n",
    "pred = model.predict(single_im)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a pretrained Convnet model\n",
    "\n",
    "In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, in image classification, it is common to use networks pre-trained on a large dataset (such as ImageNet)  and to use it either as an initialization of as a fixed feature extractor for the task of interest (**transfer learning**). Indeed, these networks have already learned features that are useful for most computer vision problems, and leveraging such features would allow us to reach a better accuracy than any method that would only rely on the available data.\n",
    "\n",
    "Different strategies can be used in transfer learning scenarios :\n",
    "\n",
    "1. The ConvNet, trained on a large image dataset such as Imagenet, is used as a fixed feature extractor. In this case, the pipeline consists in taking the pre-trained ConvNet, removing the last fully connected layer and that by treating the rest of the ConvNet architecture as a fixed feature extractor for the new dataset\n",
    "2. Fine Tuning of the ConvNet. In this case,  the weights of a part of the pretrained network are fine-tuned by continuing the backpropagation. As it as been observed that the first features of a ConvNet contain more generic features (e.g. edge detectors or color blob detectors) that should be useful to many tasks and that later layers become progressively more specific to the details of the classes contained in the original dataset, only a higher portion of the network is fine-tuned.\n",
    "\n",
    "\n",
    "### ConvNet as a fixed feature extractor   \n",
    "\n",
    "In our case, the ImageNet dataset contains several \"cat\" classes (persian cat, siamese cat...) and many \"dog\" classes among its total of 1000 classes. As a consequence any model pre-trained on ImageNet will already have learned features that are relevant to our classification problem. \n",
    "\n",
    "In particular, we will use the VGG16 architecture which won the 2014 Imagenet competition, and is a very simple model to create and understand. The VGG Imagenet team created both a larger, slower, slightly more accurate model (VGG 19) and a smaller, faster model (VGG 16). We will be using VGG 16 since the much slower performance of VGG19 is generally not worth the very minor improvement in accuracy.\n",
    "\n",
    "![VGG16](images/vgg16.png)\n",
    "\n",
    "Source : [https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/](https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/)\n",
    "\n",
    "In the code below, the strategy will consist in instantiating only the convolutional part of the model (using the *include_top* argument) (see the [Keras documentation on VGG16](https://keras.io/applications/#vgg16)) and in running this model on our own training and validation data once by recording the output in two numpy arrays. Then, you will train a small fully-connected model on top of the stored features.\n",
    "\n",
    "Some references :\n",
    " + VGG models : [http://www.robots.ox.ac.uk/~vgg/research/very_deep/](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = 'data/DogCatChallenge/sampleDeep/train'\n",
    "validation_data_dir = 'data/DogCatChallenge/sampleDeep/valid'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# Function that instanciates the convolutional part of the VGG16 pre-trained model on Imagenet and that runs it on our training and validation data\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build and load the VGG16 network without the fully connected layers\n",
    "    model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    # preparation of the training data\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    # Generation of the predictions for the input samples from the training data generator and return them as a numpy array that we can saved\n",
    "    bottleneck_features_train = model.predict(generator)\n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    # preparation of the validation data\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        #batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    # Generation of the predictions for the input samples from the validation data generator and return them as a numpy array that we can saved\n",
    "    bottleneck_features_validation = model.predict(generator)\n",
    "    np.save(open('bottleneck_features_validation.npy', 'wb'),\n",
    "            bottleneck_features_validation)\n",
    "    \n",
    "    \n",
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that trains a small fully-connected model on top of the stored previous features\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy','rb'))\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
    "\n",
    "    # Building of the small fully-connected model\n",
    "    model = Sequential([\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'), \n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Configuration of the learning process\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        steps_per_epoch = 20, \n",
    "        epochs = epochs,   # dataset relativement petit\n",
    "        batch_size = batch_size,\n",
    "        verbose = 1,\n",
    "        validation_data = (validation_data,validation_labels),callbacks=[tensorboard_callback]\n",
    "\n",
    "    )\n",
    "    \n",
    "    model.save('bottleneck_fc_model')\n",
    "    \n",
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that trains a small fully-connected model on top of the stored previous features\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy','rb'))\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
    "\n",
    "    # Building of the small fully-connected model\n",
    "    model = Sequential([\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'), \n",
    "        # on peut carr√©ment rajouter du dropout vu l'overfitting\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Configuration of the learning process\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # on peut √©galement rajouter de la r√©gularisation dans la fonction de co√ªt pour diminuer overfitting\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels, \n",
    "        epochs = epochs,   # dataset relativement petit\n",
    "        batch_size = batch_size,\n",
    "        verbose = 1,\n",
    "        validation_data = (validation_data,validation_labels),\n",
    "        callbacks=[tensorboard_callback]\n",
    "    )\n",
    "    \n",
    "    model.save('bottleneck_fc_model2')\n",
    "    \n",
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les avantages : apprentissage ultra rapide ; le probl√®me : on overfit sur le training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carefully have a look on the results and on the diffferent metrics and their obtained values. What is your interpretation of the results ?\n",
    "Try to apply this model of some unknown images, even images without dogs ar cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the top layers of a a pre-trained network\n",
    "\n",
    "We will now try to \"fine-tune\" the last convolutional block of the VGG16 model. It consist in starting from a trained network (the VGG16 network), then re-training it on a new dataset using very small weight updates. In our case, this can be done in 3 steps:\n",
    "+ Instantiate the convolutional base of VGG16 and load its weights.\n",
    "+ Add our previously defined fully-connected model on top, and load its weights.\n",
    "+ Freeze the layers of the VGG16 model up to the last convolutional block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "import keras\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = '../keras/examples/vgg16_weights.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "backend.image_data_format()\n",
    "\n",
    "train_data_dir = 'data/DogCatChallenge/sampleDeep/train'\n",
    "validation_data_dir = 'data/DogCatChallenge/sampleDeep/valid'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# creation of the base VGG pre-trained model\n",
    "model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_width,img_height,3))\n",
    "print('Model loaded.')\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "\"\"\"\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten())\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dense(128, activation='relu'))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\"\"\"\n",
    "# on fait autrement\n",
    "\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model = keras.models.load_model('bottleneck_fc_model')\n",
    "#top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "\n",
    "# creation of a real model from vgg\n",
    "new_model = Sequential()\n",
    "for l in model.layers:\n",
    "    new_model.add(l)\n",
    "\n",
    "\n",
    "# concatenation of the base model with the top model\n",
    "new_model.add(top_model)\n",
    "\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "#NON\n",
    "#for layer in new_model.layers[:25]:\n",
    "#    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "new_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n",
    "\n",
    "history = new_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = 20,  \n",
    "    epochs = 5,\n",
    "    verbose = 1,\n",
    "    batch_size = batch_size,\n",
    "    validation_data = validation_generator,\n",
    "    callbacks=[tensorboard_callback]\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout est alors beaucoup plus long (vu que m√™me un forward pass prend longtemps... m√™me la validation est longue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carefully have a look on the results and on the diffferent metrics and their obtained values. What is your interpretation of the results ?\n",
    "Try to apply this model of some unknown images, even images without dogs ar cats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "ima=io.imread(img_path)\n",
    "imshow(ima)\n",
    "\n",
    "pred = new_model.predict(single_im)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Part 3\n",
    "\n",
    "    \n",
    "+ <span style=\"background-color:lightgreen\">Introduction to representation learning </span>.\n",
    "+ <span style=\"background-color:lightgreen\"> Introduction to deep learning and CNN.</span> \n",
    "+ <span style=\"background-color:lightgreen\"> Practice on Keras </span>\n",
    "+ <span style=\"background-color:lightgreen\"> Introduction to transfer learning and fine tuning </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:lightblue\"> You have finished the part 2 ! Complete the dashboard on the global MsTeams team.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening the black box [Optional]\n",
    "\n",
    "Deep neural network models are often considered as  black-boxes and their performances come at the price of loss of interpretability. Indeed, they fail to provide explanations on their predictions. In high-risk domains, e.g., health care, or in a context of production, it is crucial to build trust in a model and being able to understand its behavior. This sub-fied of artificial intelligence is known as XAI (eXplainable Artificial Intelligence) (see for instance this [tutorial](https://sites.google.com/view/www20-explainable-ai-tutorial) or the DARPA initiative [here](https://www.darpa.mil/attachments/XAIProgramUpdate.pdf)).\n",
    "\n",
    "In particular different approaches are now well established as tools to explain a model or a model decision. You can test them on your different classification models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Maximization\n",
    "\n",
    "See this [tutorial](https://www.machinecurve.com/index.php/2019/11/18/visualizing-keras-model-inputs-with-activation-maximization/) to test it on your models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Activation maximization on your models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saliency maps\n",
    "\n",
    "See this [tutorial](https://www.machinecurve.com/index.php/2019/11/25/visualizing-keras-cnn-attention-saliency-maps/) to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Saliency maps on your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested to go deeper, a very nice tutorial is available [here](https://interpretablevision.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 : Your turn on a more challenging case\n",
    "\n",
    "Do the same job but this time on the PET Dataset : [http://www.robots.ox.ac.uk/~vgg/data/pets/](http://www.robots.ox.ac.uk/~vgg/data/pets/) which contains 37 category of pets with roughly 200 images for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources and references\n",
    "\n",
    "+ This study case is inpired from the Lesson 1 of the fast.ai's online course, Practical Deep Learning For Coders : [http://course.fast.ai/](http://course.fast.ai/)\n",
    "+ Others sources :\n",
    "    + Stanford CS231n course on Convolutional Neural Networks for Visual Recognition : [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/)\n",
    "    + Keras blog post on building image classification models [here](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
