{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "%matplotlib inline\n",
    "\n",
    "# Used to standardize volume of audio clip\n",
    "def match_target_amplitude(sound, target_dBFS):\n",
    "    change_in_dBFS = target_dBFS - sound.dBFS\n",
    "    return sound.apply_gain(change_in_dBFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Creating a dataset for audio samples\n",
    "\n",
    "Let's start by building a dataset for training our model for trigger word detection. A speech dataset should ideally be as close as possible to the application we will want to run it on, in our case we'd like to detect the word \"activate\" in working environments (library, home, offices, open-spaces ...) so we must create recordings with a mix of positive words (\"activate\") and negative words (random words other than activate) on different background sounds, so let's create a representative dataset for oue task.\n",
    "\n",
    "## 1.1 - Listening to the data   \n",
    "\n",
    "In the files provided for this lab, we have a number of recording of background sounds in different places, like libraries, cafes, restaurants, homes and offices, as well as snippets of audio of people saying positive/negative words with different accents. \n",
    "\n",
    "These recordings can be found in the `raw_data` directory, where we have a number of raw audio files of the positive words, negative words, and background noise. We will use these audio files to synthesize a dataset to train the model.\n",
    "\n",
    "- The \"activate\" directory contains positive examples of people saying the word \"activate\" (one word per audio recording). \n",
    "- The \"negatives\" directory contains negative examples of people saying random words other than \"activate\" (one word per audio recording). \n",
    "- The \"backgrounds\" directory contains 10 second clips of background noise in different environments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to listen to some examples using `IPython.display.Audio(PATH)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to some activates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to some negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Listen to some background noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use these three type of recordings (positives/negatives/backgrounds) to create a labelled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - From audio recordings to spectrograms\n",
    "\n",
    "The provided recordings are sampled at 44100 Hz. This means the microphone gives us 44100 numbers per second. Thus, a 10 second audio clip is represented by 441000 numbers (= $10 \\times 44100$). \n",
    "\n",
    "It is quite difficult to figure out from this \"raw\" representation of audio whether the word \"activate\" was said. In  order to help our sequence model more easily learn to detect triggerwords, we will compute a *spectrograms* of the audio. Spectrograms are computed by sliding a window over the raw audio signal, and calculates the most active frequencies in each window using a Fourier transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to convert raw audio to Spectrograms, we will use these hyperparameters:\n",
    "- Length of each window segment `nfft = 200`\n",
    "- Sampling frequencies `fs = 8000`\n",
    "- Overlap between windows `noverlap = 120`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plt.specgram, with:\n",
    "# If the audio data have two channels only use one given that they are equal\n",
    "def graph_spectrogram(wav_file):\n",
    "    _, data = wavfile.read(wav_file)\n",
    "    ## .....\n",
    "    pxx, freqs, bins, im = plt.specgram()\n",
    "    return pxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"audio_examples/example_train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = graph_spectrogram(\"audio_examples/example_train.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above represents how active each frequency is (y axis) over a number of time-steps (x axis). \n",
    "\n",
    "The dimension of the output spectrogram depends upon the hyperparameters of the spectrogram and the length of the input. In this notebook, we will be working with 10 second audio clips as the \"standard length\" for our training examples. The number of timesteps of the spectrogram will be 5511.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"audio_examples/example_train.wav\"\n",
    "_, data = wavfile.read(audio_file)\n",
    "x = graph_spectrogram(audio_file)\n",
    "print(\"Time steps in audio recording before spectrogram\", data[:,0].shape)\n",
    "print(\"Time steps in input after spectrogram\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the results above, we can define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = # The number of time steps input to the model from the spectrogram\n",
    "n_freq =  # Size of each input at each time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with 10 seconds of discretized samples of the audio, sampled at 441000 frequency (raw audio), we transformed it from 1D signal of size 441000 into a 2D signal as a spectrogram of size `[Tx, n_freq]`.\n",
    "\n",
    "So the key vales are:\n",
    "\n",
    "- $441000$ (raw audio frequency)\n",
    "- $5511 = T_x$ (spectrogram output, and dimension of input to RNN). \n",
    "- $10000$ (used by the `pydub` module to synthesize audio) \n",
    "- $1375 = T_y$ (the number of steps in the output of the RNN). \n",
    "\n",
    "Each of these representations correspond to exactly 10 seconds of time.\n",
    "\n",
    "In our case, we will output $T_y = 1375$ predictions for each 10s input, so for each $10/1375 \\approx 0.0072$ we will predict if someone recently finished saying \"activate.\" \n",
    "\n",
    "Consider also the 10000 number above. This corresponds to discretizing the 10sec clip into 10/10000 = 0.001 second itervals. 0.001 seconds is also called 1 millisecond, or 1ms. So when we say we are discretizing according to 1ms intervals, it means we are using 10,000 steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ty =  # The number of time steps in the output of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Generating a single training example\n",
    "\n",
    "Because speech data is hard to acquire and label, we will synthesize the training set using the audio clips of activates, negatives, and backgrounds, it is quite slow to record lots of 10 second audio clips with random \"activates\" in it, so instead we have audio samples of positives and negative words, and samples of background noise separately, and then to create a single training example, we will:\n",
    "\n",
    "- Pick a random 10 second background audio clip\n",
    "- Randomly insert 0-4 audio clips of \"activate\" into this 10sec clip\n",
    "- Randomly insert 0-2 audio clips of negative words into this 10sec clip\n",
    "\n",
    "And by knowing exactly where we added the activate clip, we can create the labels at the same time, for this we will use the pydub package to manipulate audio. Pydub converts raw audio files into lists of Pydub data structures. Pydub uses 1ms as the discretization interval which is why a 10sec clip is always represented using 10,000 steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's first low the samples we have at `raw_data`, please complete the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw audio files for speech synthesis\n",
    "def load_raw_audio():\n",
    "    activates = []\n",
    "    backgrounds = []\n",
    "    negatives = []\n",
    "    # Use AudioSegment.from_wav(wav_file_path)\n",
    "    return activates, negatives, backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio segments using pydub \n",
    "activates, negatives, backgrounds = load_raw_audio()\n",
    "\n",
    "print(\"background len: \" + str(len(backgrounds[0])))    # Should be 10,000, since it is a 10 sec clip\n",
    "print(\"activate[0] len: \" + str(len(activates[0])))     # Maybe around 1000, since an \"activate\" audio clip is usually around 1 sec (but varies a lot)\n",
    "print(\"activate[1] len: \" + str(len(activates[1])))     # Different \"activate\" clips can have different lengths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overlaying positive/negative words on the background**:\n",
    "\n",
    "So the objective is:\n",
    "- Add some activate clips (0-4) clips with no overlap between them,\n",
    "- Add some negatives (0-2) clips with no overlap between them,\n",
    "- The total length will always equal to 10s,\n",
    "- The labels $y^{\\langle t \\rangle}$ of size $T_y = 1375$, will be equal 0 in the start, and each time we add a new activate clip we update the labels in the correct position for the correct number of steps (do the correct conversion between the input step and the corresponding output step).\n",
    "\n",
    "Here's a figure illustrating the labels $y^{\\langle t \\rangle}$, for a clip which we have inserted \"activate\", \"innocent\", activate\", \"baby.\" Note that the positive labels \"1\" are associated only with the positive words. \n",
    "\n",
    "<img src=\"images/label_diagram.png\" style=\"width:500px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the training set, we will need to implement the following functions:\n",
    "    \n",
    "1. `get_random_time_segment(segment_ms)` gets a random time segment in our background audio\n",
    "2. `is_overlapping(segment_time, existing_segments)` checks if a time segment overlaps with existing segments\n",
    "3. `insert_audio_clip(background, audio_clip, existing_times)` inserts an audio segment at a random time in our background audio using `get_random_time_segment` and `is_overlapping`\n",
    "4. `insert_ones(y, segment_end_ms)` inserts 1's into our label vector y after the word \"activate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_time_segment(segment_ms):\n",
    "    \"\"\"\n",
    "    The function  returns a random time segment of size `segment_ms`\n",
    "    onto which we can insert an audio clip of duration \n",
    "    \"\"\"\n",
    "    \n",
    "    return (segment_start, segment_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `is_overlapping(segment_time, existing_segments)` to check if a new time segment overlaps with any of the previous segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_overlapping(segment_time, previous_segments):\n",
    "    \"\"\"\n",
    "    Checks if the time of a segment overlaps with the times of existing segments.\n",
    "    Returns True if the time segment overlaps with any of the existing segments, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "overlap1 = is_overlapping((950, 1430), [(2000, 2550), (260, 949)])\n",
    "overlap2 = is_overlapping((2305, 2950), [(824, 1532), (1900, 2305), (3424, 3656)])\n",
    "print(\"Overlap 1 = \", overlap1) # Must be False\n",
    "print(\"Overlap 2 = \", overlap2) # Must be True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implement `insert_audio_clip()` to overlay an audio clip onto the background 10sec clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_audio_clip(background, audio_clip, previous_segments):\n",
    "    \"\"\"\n",
    "    Insert a new audio segment over the background noise at a random time step, ensuring that the \n",
    "    audio segment does not overlap with existing segments.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_background = background.overlay(--, position = --)\n",
    "    \n",
    "    return new_background, segment_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "test_activate = './raw_data/activates/3_act2.wav'\n",
    "test_bg = './raw_data/backgrounds/2.wav'\n",
    "audio_clip, segment_time = insert_audio_clip(test_bg, test_activate, [(3790, 4400)])\n",
    "audio_clip.export(\"insert_test.wav\", format=\"wav\")\n",
    "print(\"Segment Time: \", segment_time) # Must be (2254, 3169) if you use np.random\n",
    "IPython.display.Audio(\"insert_test.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `insert_ones()`, where we get the labels (a vector of size 1375), and add 50 ones in the correct starting position (note that the input in of size 10,000, so do the correct conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_ones(y, segment_end_ms):\n",
    "    \"\"\"\n",
    "    Update the label vector y. The labels of the 50 output steps strictly (emphisis on strictly) after the end of the segment \n",
    "    should be set to 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = insert_ones(np.zeros((1, Ty)), 9700)\n",
    "plt.plot(insert_ones(arr1, 4251)[0,:])\n",
    "print(\"sanity checks:\", arr1[0][1333], arr1[0][634], arr1[0][635]) # Must be 0.0 1.0 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use `insert_audio_clip` and `insert_ones` to create a new training example.\n",
    "\n",
    "1. Initialize the label vector $y$ as a numpy array of zeros and shape $(1, T_y)$.\n",
    "2. Initialize the set of existing segments to an empty list.\n",
    "3. Randomly select 0 to 4 \"activate\" audio clips, and insert them onto the 10sec clip. Also insert labels at the correct position in the label vector $y$.\n",
    "4. Randomly select 0 to 2 negative audio clips, and insert them into the 10sec clip. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_example(background, activates, negatives):\n",
    "    \"\"\"\n",
    "    Creates a training example with a given background, activates, and negatives.\n",
    "    \"\"\"\n",
    "    np.random.seed(18) # Setting the random seed\n",
    "    background = background - 20 # Making background quieter\n",
    "\n",
    "    #### Add your code here\n",
    "    \n",
    "    background = match_target_amplitude(background, -20.0) # Standardize the volume of the audio clip \n",
    "    file_handle = background.export(\"train\" + \".wav\", format=\"wav\") # Export new training example \n",
    "    print(\"File (train.wav) was saved in your directory.\")\n",
    "    x = graph_spectrogram(\"train.wav\") # Convert to spectogram\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = create_training_example(backgrounds[0], activates, negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can listen to the training example you created and compare it to the spectrogram generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Beginning of the first activate at: {np.where(y > 0)[1][0]}') # Must be 337\n",
    "print(f'Beginning of the second activate at: {np.where(y > 0)[1][50]}') # Must be 522"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can plot the associated labels for the generated training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y[0]) # Must have two picks, at ~ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Loading the train and val sets\n",
    "\n",
    "We've now implemented the code needed to generate a single training example, the same approch was used to create the set in `XY_train/` and `XY_dev/`, let's load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed training examples\n",
    "X = np.load(\"./XY_train/X.npy\")\n",
    "Y = np.load(\"./XY_train/Y.npy\")\n",
    "# Load preprocessed val set examples\n",
    "X_val = np.load(\"./XY_val/X_val.npy\")\n",
    "Y_val = np.load(\"./XY_val/Y_val.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Build the model\n",
    "\n",
    "Here is the architecture we will use. Take some time to look over the model and see if it makes sense. \n",
    "<img src=\"images/model.png\" style=\"width:1000px;height:1000px;\">\n",
    "\n",
    "The model takes as inputs 5511 step spectrogram, so first we must use a 1D conv to go from Tx = 5511 to Ty = 1375, and then use two layers of a recurrent net to output the predictions.\n",
    "\n",
    "Note that we use a uni-directional RNN rather than a bi-directional RNN. This is really important for trigger word detection, since we want to be able to detect the trigger word almost immediately after it is said. If we used a bi-directional RNN, we would have to wait for the whole 10sec of audio to be recorded before we could tell if \"activate\" was said in the first second of the audio clip.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the model can be done in four steps:\n",
    "\n",
    "\n",
    "- For the CONV layer. Use `Conv1D()` to implement this, with 196 filters, with a filter size of 15 (`kernel_size=15`), **find the correct stride to for an output of size 1375 with an input of size 5511**. [[See documentation.](https://keras.io/layers/convolutional/#conv1d)]\n",
    "\n",
    "- For the two GRU layers, use: [[See documentation.](https://keras.io/layers/recurrent/#GRU)].\n",
    "\n",
    "- Create a time-distributed dense layer as follows: `X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X)`. This creates a dense layer followed by a sigmoid, so that the parameters used for the dense layer are the same for every time step and the output between 0 and 1. [[See documentation](https://keras.io/layers/wrappers/).]\n",
    "\n",
    "Implement `model()`, the architecture is presented in Figure 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    Returns a Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input = Input(...)\n",
    "\n",
    "    # CONV layer\n",
    "    # First GRU Layer\n",
    "    # Second GRU Layer\n",
    "\n",
    "    \n",
    "    # Time-distributed dense layer\n",
    "    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X) # time distributed  (sigmoid)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(input_shape = (Tx, n_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the model summary to keep track of the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "'''\n",
    "We must have as ouput:\n",
    "Total params: 522,561\n",
    "Trainable params: 521,657\n",
    "Non-trainable params: 904\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the network is of shape (None, 1375, 1) while the input is (None, 5511, 101). The Conv1D has reduced the number of steps from 5511 at spectrogram to 1375. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger word detection takes a long time to train. To save time, we've already trained a model for about 3 hours on a GPU using the architecture you built above, and a large training set of about 4000 examples. Let's load the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models/\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train the model further, using the Adam optimizer and binary cross entropy loss, as follows. This will run quickly because we are training just for one epoch and with a small training set of 26 examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer (use Adam), and pass the parameters (learning rate, momentum values and decay rate)\n",
    "opt = ..\n",
    "# Compile the model, use loss binary CE and the metric as accuracy\n",
    "model.compile(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using a batch of 5 and one epoch\n",
    "model.fit(...) # Accuracy at the end must be ~ 98 / 97.5 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Test the model\n",
    "\n",
    "Finally, let's see how your model performs on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_val, Y_val)\n",
    "print(\"Dev set accuracy = \", acc) # Must be in the range of 92 - 95 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good! However, accuracy isn't a great metric for this task, since the labels are heavily skewed to 0's, so a neural network that just outputs 0's would get slightly over 90% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(...)\n",
    "\n",
    "def f1_score(Y_pred, Y_val, threshold):\n",
    "\n",
    "    return f1_score\n",
    "    \n",
    "f1_score(Y_pred, Y_val, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Making Predictions\n",
    "\n",
    "Now that you have built a working model for trigger word detection, let's use it to make predictions. For this implements a function that computes the spectogram of an input audio clip, swap the axes ((freqs, Tx) to (Tx, freqs) for the model), and pass it through the network to get the predictions, and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_triggerword(filename):\n",
    "    plt.subplot(2, 1, 1)\n",
    "\n",
    "    # preprocessing\n",
    "    predictions = # predict\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(predictions[0,:,0])\n",
    "    plt.ylabel('probability')\n",
    "    plt.show()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've estimated the probability of having detected the word \"activate\" at each output step, we can trigger a \"bell\" sound to play when the probability is above a certain threshold. Further, $y^{\\langle t \\rangle}$ might be near 1 for many values in a row after \"activate\" is said, yet we want to add ringing sound only once. So we will insert a ring sound at most once every 75 output steps. This will help prevent us from inserting two ring sounds for a single instance of \"activate\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ring_file = \"audio_examples/ring.wav\"\n",
    "def ring_on_activate(filename, predictions, threshold):\n",
    "    # open both wav files\n",
    "    audio_clip = \n",
    "    ring = \n",
    "    # if output is 1 for 75 consecutive output steps, add a ring sound\n",
    "    # superpose audio and background using pydub (audio_clip.overlay)\n",
    "    # ...\n",
    "    audio_clip.export(\"ring_output.wav\", format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Test examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore how our model performs on two unseen audio clips from the development set. Lets first listen to the two dev set clips. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/test/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/test/2.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the model on these audio clips and see if it adds a ring sounds after \"activate\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./raw_data/test/1.wav\"\n",
    "# Predict & add bell counds\n",
    "# Head the results\n",
    "IPython.display.Audio(\"./ring_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename  = \"./raw_data/test/2.wav\"\n",
    "# Predict & add bell counds\n",
    "# Head the results\n",
    "IPython.display.Audio(\"./ring_output.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Try your own example\n",
    "\n",
    "\n",
    "Record a given audio clip of you saying the word \"activate\" and other random words. Be sure to use the audio as a wav file. If your audio is recorded in a different format (such as mp3) there is free software that you can find online for converting it to wav.\n",
    "\n",
    "Now, your recording can be larger or smaller that 10 seconds, complete the code below to trim or pad it as needed to make it 10 seconds. \n",
    "\n",
    "Use the correct functions that can be found here [Pydub Docs](https://www.pydoc.io/pypi/pydub-0.9.5/autoapi/audio_segment/index.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio to the correct format\n",
    "def preprocess_audio(filename):\n",
    "    # Trim -> pad -> set frame rate to 44100\n",
    "    segment = \n",
    "    segment.export(filename, format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_filename = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio(your_filename)\n",
    "IPython.display.Audio(your_filename) # listen to the audio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the model to predict when you say activate in the 10 second audio clip, and trigger a ring bell. If beeps are not being added appropriately, try to adjust the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "prediction = detect_triggerword(your_filename)\n",
    "ring_on_activate(your_filename, prediction, threshold)\n",
    "IPython.display.Audio(\"./ring_output.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Use Conv nets as your model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we first transformed our raw audio into a spectrogram, created a dataset and then used a recurrent model for detecting the trigger words, one possible alternative is to convert the spectrograms into MFCCs (Mel-frequency cepstrum), create a dataset of images, with the correct labels, and construct a CNN for detecting the trigger words, either a binary detection (in the audio clip or not), or even output a vector indicating the location of the trigger words, it is up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step1:** Transform the training / val samples (from spectrograms) into images (using MFCC)\n",
    "* **Step2:** Adjust the predictions / labels for a computer vision task\n",
    "* **Step3:** Save images & labels\n",
    "* **Step4:** Create a convnet\n",
    "* **Step5:** Train & Test\n",
    "\n",
    "Tutorials:\n",
    "https://www.kaggle.com/davids1992/speech-representation-and-data-exploration\n",
    "https://www.kaggle.com/alexozerin/end-to-end-baseline-tf-estimator-lb-0-72"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "rSupZ",
   "launcher_item_id": "cvGhe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
